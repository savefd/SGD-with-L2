{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 1: Training Model</center></h1><br>\n",
    "<h3>Napong Leelasithorn 61070505228</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Tasks [10 points] </b>\n",
    "<ol>\n",
    "    <li>[1 point] Prepare the data in one-against-the-rest strategy. This can be done by converting the \"Species\" column into 3 binary columns</li>\n",
    "    <li>[2 points] Formulate the error function of the logistic regression with ridge regularization criterion.</li>\n",
    "    <li>[2 points] Derive the gradient of the error function of the logistic regression with ridge regularization criterion.</li>\n",
    "    <li>[2 points] Implement the gradient descent using all of the dataset in each iteration. (Use equation from Task 3)</li>\n",
    "    <li>[1 point] Implement the stochastic gredient descent using the subset of dataset in each iteration. (use equation from Task 3)</li>\n",
    "    <li>[1 point] Test to see the effect of $\\lambda$ on the training process.</li>\n",
    "    <li>[1 point] Test to see the effect of sampling proportion in Task 5.</li> \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 [1 point]\n",
    "#### Load Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('IRIS.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intercept  sepal_length  sepal_width  petal_length  petal_width  \\\n",
       "0          1           5.1          3.5           1.4          0.2   \n",
       "1          1           4.9          3.0           1.4          0.2   \n",
       "2          1           4.7          3.2           1.3          0.2   \n",
       "3          1           4.6          3.1           1.5          0.2   \n",
       "4          1           5.0          3.6           1.4          0.2   \n",
       "\n",
       "   Iris-setosa  Iris-versicolor  Iris-virginica  \n",
       "0            1                0               0  \n",
       "1            1                0               0  \n",
       "2            1                0               0  \n",
       "3            1                0               0  \n",
       "4            1                0               0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.concat([iris, pd.get_dummies(iris['species'])], axis = 1).drop(columns = 'species')\n",
    "iris.insert(0, 'intercept', 1)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train = iris.sample(frac = 0.8, random_state = 1)\n",
    "iris_test = iris.drop(index = iris_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform y into np.array format\n",
    "# one-against-the-rest\n",
    "\n",
    "# train\n",
    "y_set_train = np.array(iris_train[['Iris-setosa']]) # setosa = 1\n",
    "y_ver_train = np.array(iris_train[['Iris-versicolor']]) # versicolor = 1\n",
    "y_vir_train = np.array(iris_train[['Iris-virginica']]) # virginica = 1\n",
    "\n",
    "# test\n",
    "y_set_test = np.array(iris_test[['Iris-setosa']]) # setosa = 1\n",
    "y_ver_test = np.array(iris_test[['Iris-versicolor']]) # versicolor = 1\n",
    "y_vir_test = np.array(iris_test[['Iris-virginica']]) # virginica = 1\n",
    "\n",
    "y_set_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 5.8, 4. , 1.2, 0.2],\n",
       "       [1. , 5.1, 2.5, 3. , 1.1],\n",
       "       [1. , 6.6, 3. , 4.4, 1.4],\n",
       "       [1. , 5.4, 3.9, 1.3, 0.4],\n",
       "       [1. , 7.9, 3.8, 6.4, 2. ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform all features into np.array format\n",
    "x_train = np.array(iris_train.iloc[:,:5])\n",
    "x_test = np.array(iris_test.iloc[:,:5])\n",
    "# add an intercept\n",
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logistic regression</b>\n",
    "$$ H_w(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$ <br> \n",
    "Where\n",
    "$$z = f(x,w) = \\sum_{i = 1}^{n} w_i x_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the sigmoid function as (1)\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error function for ridge regression can be expressed in the form of (cost function + L2 regularization term)<br><br>\n",
    "$$ E(w) = L(w) + \\lambda\\sum_{i = 1}^n w_i^2 $$<br>\n",
    "and $L(w)$ is,<br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    "L(w) &=  \\frac{1}{n}\\sum_{i = 1}^{n}[{-y_i}\\log{H_w}({x_i}) - (1 - {y_i})\\log(1 - {H_w}({x_i}))] \\\\ \n",
    "&= \\frac{-1}{n}\\sum_{i = 1}^{n}[{y_i}\\log{H_w}({x_i}) + (1 - {y_i})\\log(1 - {H_w}({x_i}))] \n",
    "\\end{align}\n",
    "$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, <br>\n",
    "$$ E(w) = \\frac{-1}{n}\\sum_{i = 1}^{n}[{y_i}\\log{H_w}({x_i}) + (1 - {y_i})\\log(1 - {H_w}({x_i}))] + \\lambda\\sum_{i = 1}^n w_i^2 \\tag{2} $$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate this error function using matrix form instead of processing a single row, like this <br><br><br>\n",
    "$$E(w) = \\frac{-1}{n} \\times \\left(\\mathbf{Y}^T \\cdot log(\\mathbf{H_w(X\\cdot{W})}) + \\mathbf{(1-Y)}^T \\cdot log(\\mathbf{1-H_w(X\\cdot{W})}) \\right) + \\lambda (\\mathbf{W}^T\\mathbf{W}) $$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulate the error function.\n",
    "def errorfunction(x, y, w, _lambda):\n",
    "    n = len(x)\n",
    "    h = sigmoid(x @ w)\n",
    "    return (-1/n)*((y.transpose() @ np.log(h)) + (1-y).transpose() @ np.log(1-h)) \\\n",
    "            + (_lambda*(w.transpose() @ w)), h  # return the error function and Sigmoid of (xw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "$$ \\frac{-1}{n}  \\times\n",
    "\\left (\n",
    "\\begin{bmatrix} \n",
    "y_1 & y_2 & {..} & y_n\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "log(H_w(f(x_1,w_1)))\\\\\n",
    "log(H_w(f(x_2,w_2)))\\\\\n",
    "...\\\\\n",
    "log(H_w(f(x_n,w_n)))\n",
    "\\end{bmatrix}\n",
    "\\right )\n",
    "\\times\n",
    "\\begin{bmatrix} \n",
    "{1 - y_1} & 1-y_2 & {..} & 1-y_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1-log(H_w(f(x_1,w_1)))\\\\\n",
    "1-log(H_w(f(x_2,w_2)))\\\\\n",
    "...\\\\\n",
    "1-log(H_w(f(x_n,w_n)))\n",
    "\\end{bmatrix} + \\lambda\n",
    "\\begin{bmatrix} \n",
    "w_1 & w_2 & {..} & w_n\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "w_1 \\\\ w_2 \\\\ {..} \\\\ w_n\n",
    "\\end{bmatrix}\n",
    "$$ <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the <b>equation (2)</b> to find the partial derivative of the error function in <b>Task 2</b> <br>\n",
    "Let's derive the gradient of the $E(w)$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E(w) = \\frac{-1}{n}\\sum_{i = 1}^{n}[{y_i}\\log{H_w}({x_i}) + (1 - {y_i})\\log(1 - {H_w}({x_i}))] + \\lambda\\sum_{i = 1}^n w_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then<br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E(w)}{\\partial w} &= \\frac{\\partial}{\\partial w}(\\frac{-1}{n}\\sum_{i = 1}^{n}[{y_i}\\log{H_w}({x_i}) + (1 - {y_i})\\log(1 - {H_w}({x_i}))] + \\lambda\\sum_{i = 1}^n w_i^2) \\\\\n",
    "&= \\frac{-1}{n}\\left(\\frac{\\partial}{\\partial w}\\sum_{i = 1}^{n}[{y_i}\\log{H_w}({x_i}) + (1 - {y_i})\\log(1 - {H_w}({x_i}))] \\right) + \\lambda\\frac{\\partial}{\\partial w}\\sum_{i = 1}^n w_i^2\\tag{3}\n",
    "\\end{align}\n",
    "$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will apply the partial derivertive into these 2 terms in (3) separately.<br>\n",
    "For the first term, we should calculate the derivative of sigmoid function (1) before take a look at the derivative of cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function\n",
    "$$\n",
    "H(z) =  \\frac{1}{1+\\exp^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diff,\n",
    "$$ H(z)' = \\left( \\frac{1}{1+\\exp^{-z}} \\right)' = \\frac{-(1+e^{-z})'}{(1+e^{-z})^2} = \\frac{0- (e^{-z})'}{(1+e^{-z})^2} = \\frac{-(-z)'(e^{-z})}{(1+e^{-z})^2} = \\frac{1(e^{-z})}{(1+e^{-z})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this step, if it is partial derivative with respect to \"w\", because of the chain rule there will be an additional multiplication term which is $x$ from the partial derivertive of $f(x,w)$ respect to $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue\n",
    "$$\n",
    "\\begin{align}\n",
    "H(z)' &= \\frac{e^{-z}}{(1 + e^{-z})^2} \\\\\\\\\n",
    "& = \\left( \\frac{1}{1 + e^{-z}} \\right ) \\left ( \\frac{e^{-z}}{1 + e^{-z}} \\right ) \\\\\\\\\n",
    "& = H(z) \\left ( \\frac{+1-1+e^{-z}}{1+e^{-z}} \\right ) \\\\\\\\\n",
    "& = H(z) \\left ( \\frac{1+e^{-z}}{1+e^{-z}} - \\frac{1}{1+e^{-z}} \\right ) \\\\\\\\\n",
    "& = H(z)(1 - H(z)) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply the above equation with $x_i$ term for the partial derivative with respect to $w$ <br>\n",
    "Therefore, we will get this <br><br>\n",
    "$$ \\frac{\\partial}{\\partial w}H_w(x_i) = H_w(x_i)(1-H_w(x_i))x_i \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will try to apply the partial derivative with the cost/loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(w)}{\\partial w} &= \\frac{-1}{n}\\left(\\frac{\\partial}{\\partial w}\\sum_{i = 1}^{n}[{y_i}\\log{H_w}({x_i}) + (1 - {y_i})\\log(1 - {H_w}({x_i}))]\\right) \\\\\\\\\n",
    "&= \\frac{-1}{n}\\sum_{i = 1}^{n}[{y_i}\\frac{\\partial}{\\partial w}\\log{H_w}({x_i})+ (1 - {y_i})\\frac{\\partial}{\\partial w}\\log(1 - {H_w}({x_i}))] \\\\\\\\\n",
    "&= \\frac{-1}{n}\\sum_{i = 1}^{n} \\left[ \\frac{{y_i}\\frac{\\partial}{\\partial w}{H_w}({x_i})}{{H_w}({x_i})} + \\frac{(1 - {y_i})\\frac{\\partial}{\\partial w}(1 - {H_w}({x_i}))}{(1 - {H_w}({x_i}))} \\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute (4), $\\frac{\\partial}{\\partial w}H_w(x_i) = H_w(x_i)(1-H_w(x_i))x_i$ in this step,<br>\n",
    "Then we will get<br><br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial L(w)}{\\partial w} &= \\frac{-1}{n}\\sum_{i = 1}^{n} \\left[ \\frac{{y_i}H_w(x_i)(1-H_w(x_i))x_i}{{H_w}({x_i})} + \\frac{-(1 - {y_i})H_w(x_i)(1-H_w(x_i))x_i}{(1 - {H_w}({x_i}))} \\right ]\\\\\\\\\n",
    "&= \\frac{-1}{n}\\sum_{i = 1}^{n}[{y_i}(1-H_w(x_i)) -(1 - {y_i})H_w(x_i)]x_i \\\\\\\\\n",
    "&= \\frac{-1}{n}\\sum_{i = 1}^{n}[y_i - y_iH_w(x_i) - H_w(x_i) + y_iH_w(x_i)]x_i \\\\\\\\\n",
    "&= \\frac{-1}{n}\\sum_{i = 1}^{n}(y_i - H_w(x_i))x_i \\\\\\\\\n",
    "&= \\frac{1}{n}\\sum_{i = 1}^{n}(H_w(x_i)-y_i)x_i \\tag{5}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the L2 regularization term.<br><br>\n",
    "$$\\lambda\\frac{\\partial}{\\partial w}\\sum_{i = 1}^n w_i^2 = 2\\lambda\\sum_{i=1}^{n}w_i\\tag{6} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine these two terms (5)(6) to get the partial derivative of $E(w)$,<br><br><br>\n",
    "$$\\frac{\\partial E(w)}{\\partial w} =\\frac{1}{n}\\sum_{i = 1}^{n}(H_w(x_i)-y_i)x_i + 2\\lambda\\sum_{i=1}^{n}w_i \\tag{*}$$<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "Implement gradient descent using equation in task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be implement by this vectorized version:<br><br>\n",
    "$$ \\triangledown E(w) = \\frac{1}{n} \\times X^T \\cdot({H(X\\cdot W)} - Y) + 2\\lambda W$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we'll update \"w\" as this equation<br><br>\n",
    "$$\\mathbf{w} = \\mathbf{w} - \\frac{\\alpha}{n} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right) + 2 \\lambda \\mathbf{w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientdescent(x, y, w, _lambda, iterations, alpha):\n",
    "    n = len(x)\n",
    "    for i in range(0, iterations):\n",
    "        loss,h = errorfunction(x,y,w,_lambda)\n",
    "        w = w - alpha*(x.transpose() @ (h-y)) + 2*_lambda*w\n",
    "        print(\"Error:{}\".format(float(loss)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent.<br>\n",
    "From Task 7 want me to test the sampling proportion from this task, <br>\n",
    "So, I decide to implement mini-batch gradient descent instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatchGD(x, y, w, _lambda, iterations, alpha, numsampling):\n",
    "    n = len(x)\n",
    "    for i in range(0, iterations):\n",
    "        \n",
    "        # create an array which has length = number of row x\n",
    "        index = np.arange(x.shape[0])\n",
    "        \n",
    "        # shuffle index\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        # match the index which already shuffled with x and y, and the number of samples = numsampling\n",
    "        x_ran, y_ran = x[index[:numsampling+1]], y[index[:numsampling+1]]\n",
    "        \n",
    "        loss,h = errorfunction(x_ran,y_ran,w,_lambda)\n",
    "        w = w - alpha*(x_ran.transpose() @ (h-y_ran)) + 2*_lambda*w\n",
    "        print(\"Error:{}\".format(float(loss)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "### Task 6\n",
    "Test to see the effect of $\\lambda$ on training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize w with the all-ones matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.ones((x_train.shape[1],1))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy test for gradient descent gradient descent\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def accuracy_test(x, y, w):\n",
    "    h = sigmoid(x@w)\n",
    "    h = np.where(h > 0.5, 1., h)\n",
    "    h = np.where(h < 0.5, 0., h)\n",
    "    return classification_report(y, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know that, $\\lambda$ term can prevent overfitting.<br>\n",
    "So, I build 2 different models with lambda = 0 and 0.05 and other all parameters are the same.(Using normal gradient descent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train accuracy for the 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:11.27417257251455\n",
      "Error:9.221586689153398\n",
      "Error:7.1693047370714496\n",
      "Error:5.1194608705332145\n",
      "Error:3.0917231317009577\n",
      "Error:1.2840795940860943\n",
      "Error:0.5592519662689854\n",
      "Error:0.5180139113586185\n",
      "Error:0.49703386797246185\n",
      "Error:0.4780170994557533\n",
      "Error:0.46011091782392566\n",
      "Error:0.4431976727772849\n",
      "Error:0.4272177617264853\n",
      "Error:0.41211764298709735\n",
      "Error:0.3978457121489225\n",
      "Error:0.38435227561672947\n",
      "Error:0.3715897735541381\n",
      "Error:0.35951293557534003\n",
      "Error:0.348078864696324\n",
      "Error:0.3372470651815217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        80\n",
      "           1       1.00      0.85      0.92        40\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       120\n",
      "   macro avg       0.97      0.93      0.94       120\n",
      "weighted avg       0.95      0.95      0.95       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_test(x_train, y_set_train,gradientdescent(x_train, y_set_train, w,  _lambda = 0, iterations =20, alpha = 0.0005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy for the 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:11.27417257251455\n",
      "Error:9.221586689153398\n",
      "Error:7.1693047370714496\n",
      "Error:5.1194608705332145\n",
      "Error:3.0917231317009577\n",
      "Error:1.2840795940860943\n",
      "Error:0.5592519662689854\n",
      "Error:0.5180139113586185\n",
      "Error:0.49703386797246185\n",
      "Error:0.4780170994557533\n",
      "Error:0.46011091782392566\n",
      "Error:0.4431976727772849\n",
      "Error:0.4272177617264853\n",
      "Error:0.41211764298709735\n",
      "Error:0.3978457121489225\n",
      "Error:0.38435227561672947\n",
      "Error:0.3715897735541381\n",
      "Error:0.35951293557534003\n",
      "Error:0.348078864696324\n",
      "Error:0.3372470651815217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        20\n",
      "           1       1.00      0.60      0.75        10\n",
      "\n",
      "   micro avg       0.87      0.87      0.87        30\n",
      "   macro avg       0.92      0.80      0.83        30\n",
      "weighted avg       0.89      0.87      0.86        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_test(x_test, y_set_test,gradientdescent(x_train, y_set_train, w,  _lambda = 0, iterations =20, alpha = 0.0005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can see a little bit overfitting without lambda (=0).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train accuracy for the 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:11.52417257251455\n",
      "Error:10.583224299643394\n",
      "Error:9.552585390320555\n",
      "Error:8.424267594033083\n",
      "Error:7.189842037562974\n",
      "Error:5.841012017370671\n",
      "Error:4.372988399895765\n",
      "Error:2.8092069194794886\n",
      "Error:1.3984480298265627\n",
      "Error:0.9091424056762596\n",
      "Error:0.9340788207720097\n",
      "Error:1.0017104149739018\n",
      "Error:1.0899242232770134\n",
      "Error:1.201067925266464\n",
      "Error:1.3401856055549985\n",
      "Error:1.5134973252765846\n",
      "Error:1.7284276096387263\n",
      "Error:1.9938591006921964\n",
      "Error:2.320457263590746\n",
      "Error:2.7210570825593807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90        80\n",
      "           1       0.87      0.68      0.76        40\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       120\n",
      "   macro avg       0.86      0.81      0.83       120\n",
      "weighted avg       0.86      0.86      0.85       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1st model using _lambda = 0 to predict trainset\n",
    "print(accuracy_test(x_train, y_set_train,gradientdescent(x_train, y_set_train, w,  _lambda = 0.05, iterations =20, alpha = 0.0005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy for the 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:11.52417257251455\n",
      "Error:10.583224299643394\n",
      "Error:9.552585390320555\n",
      "Error:8.424267594033083\n",
      "Error:7.189842037562974\n",
      "Error:5.841012017370671\n",
      "Error:4.372988399895765\n",
      "Error:2.8092069194794886\n",
      "Error:1.3984480298265627\n",
      "Error:0.9091424056762596\n",
      "Error:0.9340788207720097\n",
      "Error:1.0017104149739018\n",
      "Error:1.0899242232770134\n",
      "Error:1.201067925266464\n",
      "Error:1.3401856055549985\n",
      "Error:1.5134973252765846\n",
      "Error:1.7284276096387263\n",
      "Error:1.9938591006921964\n",
      "Error:2.320457263590746\n",
      "Error:2.7210570825593807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        20\n",
      "           1       1.00      0.60      0.75        10\n",
      "\n",
      "   micro avg       0.87      0.87      0.87        30\n",
      "   macro avg       0.92      0.80      0.83        30\n",
      "weighted avg       0.89      0.87      0.86        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2nd model using _lambda = 0 to predict testset\n",
    "print(accuracy_test(x_test, y_set_test,gradientdescent(x_train, y_set_train, w,  _lambda = 0.05, iterations =20, alpha = 0.0005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the difference between train accuracy and test accuracy in both model is dicreasing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Therefore, adding regularization term can preventing overfit or we can just saying that it can make the model to not too fit with the data by adding some noise <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the effect of sampling proportion of mini-batch gradient descent.<br><br>\n",
    "<b>Start with number of samples = 30</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:11.423916390344004\n",
      "Error:1.1122206082662156\n",
      "Error:1.952740526136283\n",
      "Error:0.5492276305084025\n",
      "Error:0.4678279478950846\n",
      "Error:0.7277792122925285\n",
      "Error:0.21070520921059657\n",
      "Error:0.2076671628070851\n",
      "Error:0.1743499285147525\n",
      "Error:0.17176086809004715\n",
      "Error:0.1491575193852042\n",
      "Error:0.1314835613724362\n",
      "Error:0.11934516814023644\n",
      "Error:0.11501096664576202\n",
      "Error:0.11004996652339073\n",
      "Error:0.10266058116415856\n",
      "Error:0.09950076643203011\n",
      "Error:0.09209631333863712\n",
      "Error:0.09054300055382575\n",
      "Error:0.08608228540599312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numsampling = 30\n",
    "print(accuracy_test(x_test, y_set_test,minibatchGD(x_train, y_set_train, w, \n",
    "                                                   _lambda = 0.003, \n",
    "                                                   iterations =20,\n",
    "                                                   alpha = 0.003,\n",
    "                                                   numsampling = 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model seem to stucked at local minimum as it always decreasing over iterations, but I think that this dataset's too pretty. So, we cannot easily make it obvious overfitting at testset (always 1.0 for train and test accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "But if we look at this following stochastic gradient descent<br>\n",
    "<b> number of sample = 1 </b><br>\n",
    "at error value in line 2 (0.0144), it about to stucks at local minimum but the random sample help to prevent SGD going down and escape that local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:14.415000568684032\n",
      "Error:9.928000779024883\n",
      "Error:0.01449706646619383\n",
      "Error:16.52015647180086\n",
      "Error:7.990640497609038\n",
      "Error:16.255483080921714\n",
      "Error:15.970138324689685\n",
      "Error:7.578490758678858\n",
      "Error:14.759352904229212\n",
      "Error:16.095260322098333\n",
      "Error:13.36557125179011\n",
      "Error:5.927972814525737\n",
      "Error:6.52009533561195\n",
      "Error:14.323090761699204\n",
      "Error:14.07560906192835\n",
      "Error:6.391011205233222\n",
      "Error:14.043714665615875\n",
      "Error:11.274487485400167\n",
      "Error:6.174755425285734\n",
      "Error:12.81443938681103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        80\n",
      "           1       0.33      1.00      0.50        40\n",
      "\n",
      "   micro avg       0.33      0.33      0.33       120\n",
      "   macro avg       0.17      0.50      0.25       120\n",
      "weighted avg       0.11      0.33      0.17       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numsampling = 1\n",
    "print(accuracy_test(x_train, y_set_train,minibatchGD(x_train, y_set_train, w, \n",
    "                                                   _lambda = 0.003,\n",
    "                                                   iterations =20,\n",
    "                                                   alpha = 0.003,\n",
    "                                                   numsampling = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has only 33% accuracy because I use only 20 iterations in this model. If you want to find out the better one from SGD you can change another parameters like alpha, iterations to find the actual global minimum<br>\n",
    "#### like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:17.565000208303907\n",
      "Error:16.899195998737934\n",
      "Error:0.013340043214273086\n",
      "Error:8.895112928063805\n",
      "Error:14.300751981450292\n",
      "Error:0.012482415492046857\n",
      "Error:16.12267658240705\n",
      "Error:13.39106211713242\n",
      "Error:10.987255778678206\n",
      "Error:12.473460304137573\n",
      "Error:10.823815630265756\n",
      "Error:11.62019665713977\n",
      "Error:5.661552036471206\n",
      "Error:5.591279578126396\n",
      "Error:5.410158855052009\n",
      "Error:0.009136214124102308\n",
      "Error:4.328289859150251\n",
      "Error:4.305660729842606\n",
      "Error:9.669978367734753\n",
      "Error:0.012509749148180498\n",
      "Error:3.81797652902965\n",
      "Error:7.422764337750684\n",
      "Error:7.920374610097718\n",
      "Error:5.85722931077063\n",
      "Error:6.957609145647011\n",
      "Error:6.05845362194369\n",
      "Error:3.8866685808602064\n",
      "Error:1.8078535861396183\n",
      "Error:3.500307251863104\n",
      "Error:2.849932274465764\n",
      "Error:2.3766924162991745\n",
      "Error:0.2877638021372176\n",
      "Error:2.0459861247655295\n",
      "Error:1.4483062338991473\n",
      "Error:1.0284250324151092\n",
      "Error:0.5718413479952249\n",
      "Error:0.6282722418806874\n",
      "Error:0.3454219798969302\n",
      "Error:0.49265360650315826\n",
      "Error:0.6953426800183896\n",
      "Error:0.33398638308241374\n",
      "Error:0.822385777763249\n",
      "Error:0.224290446507707\n",
      "Error:0.19749655273215577\n",
      "Error:1.065642916498726\n",
      "Error:0.3714117133100582\n",
      "Error:0.6369967089161681\n",
      "Error:0.6102001913918681\n",
      "Error:0.5624815355050978\n",
      "Error:0.23888715313760528\n",
      "Error:0.5875689002875583\n",
      "Error:0.2945994263914237\n",
      "Error:0.6976345502344891\n",
      "Error:0.6588784883089263\n",
      "Error:0.4469510197302511\n",
      "Error:1.0225865643939018\n",
      "Error:0.48003926649042933\n",
      "Error:0.46830305734085237\n",
      "Error:0.5750784576796943\n",
      "Error:0.5687374887384903\n",
      "Error:0.7293283294181114\n",
      "Error:0.5715771486035255\n",
      "Error:0.36059760187781786\n",
      "Error:0.5514281858675424\n",
      "Error:0.5003146480568427\n",
      "Error:0.6538782532459081\n",
      "Error:0.2998473122283751\n",
      "Error:0.11830113574817591\n",
      "Error:0.8863397209170859\n",
      "Error:0.2875137039357504\n",
      "Error:0.771891461101572\n",
      "Error:0.46396456742842257\n",
      "Error:0.28138222173257527\n",
      "Error:0.6144239839313922\n",
      "Error:0.5071476195025938\n",
      "Error:0.4113928495759787\n",
      "Error:0.27685296565841233\n",
      "Error:0.4830852652620833\n",
      "Error:0.3414701305772165\n",
      "Error:0.6563170216477375\n",
      "Error:0.35151303262164085\n",
      "Error:0.41951500473409975\n",
      "Error:0.22681404890119122\n",
      "Error:0.2183207765721377\n",
      "Error:0.40508764469798514\n",
      "Error:0.4548364147486488\n",
      "Error:0.1142855871599588\n",
      "Error:0.4568945590540576\n",
      "Error:0.3993826341199972\n",
      "Error:0.6193946501679679\n",
      "Error:0.4624796838184625\n",
      "Error:0.5285550965067304\n",
      "Error:0.3226604019305206\n",
      "Error:0.15516774214164758\n",
      "Error:0.168534460612602\n",
      "Error:0.12071356191944267\n",
      "Error:0.638587037798303\n",
      "Error:0.617266198694738\n",
      "Error:0.4884477799937836\n",
      "Error:0.4998857576335616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        20\n",
      "           1       1.00      0.80      0.89        10\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        30\n",
      "   macro avg       0.95      0.90      0.92        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_test(x_test, y_set_test,minibatchGD(x_train, y_set_train, w, \n",
    "                                                   _lambda = 0.003, \n",
    "                                                   iterations =100,\n",
    "                                                   alpha = 0.005,\n",
    "                                                   numsampling = 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
